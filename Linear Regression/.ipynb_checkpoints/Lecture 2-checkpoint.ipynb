{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regresion - part 2\n",
    "Many variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from IPython.display import (\n",
    "    display, \n",
    "    Math, \n",
    "    Latex\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many variables we will use vectorized implementation\n",
    "$$X=\\left[\\begin{array}{cc}\n",
    "1 & (\\vec x^{(1)})^T \\\\\n",
    "1 & (\\vec x^{(2)})^T \\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & (\\vec x^{(m)})^T \\\\\n",
    "\\end{array}\\right] \n",
    "= \\left[\\begin{array}{cccc}\n",
    "1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_1^{(m)} & \\cdots & x_n^{(m)} \\\\\n",
    "\\end{array}\\right] $$\n",
    "\n",
    " $$\\vec{y} = \n",
    "\\left[\\begin{array}{c}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "y^{(m)}\\\\\n",
    "\\end{array}\\right]\n",
    "\\quad\n",
    "\\theta = \\left[\\begin{array}{c}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_n\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "\n",
    "Vectorized implementation is much faster than that one from previous Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ex1data1.txt\", header=None)\n",
    "df.columns = columns=['x', 'y']\n",
    "X = np.matrix(df.x.values[:, np.newaxis])\n",
    "# adding theta_0\n",
    "m = len(X)\n",
    "X = np.concatenate((np.ones((1,m)).T, X), axis=1)\n",
    "y = np.matrix(df.y.values[:, np.newaxis])\n",
    "theta = np.matrix([-5, 1.3]).reshape(2, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[ 1.      6.1101]\n",
      " [ 1.      5.5277]\n",
      " [ 1.      8.5186]\n",
      " [ 1.      7.0032]\n",
      " [ 1.      5.8598]\n",
      " [ 1.      8.3829]\n",
      " [ 1.      7.4764]\n",
      " [ 1.      8.5781]\n",
      " [ 1.      6.4862]\n",
      " [ 1.      5.0546]]\n",
      "y [[ 17.592 ]\n",
      " [  9.1302]\n",
      " [ 13.662 ]\n",
      " [ 11.854 ]\n",
      " [  6.8233]\n",
      " [ 11.886 ]\n",
      " [  4.3483]\n",
      " [ 12.    ]\n",
      " [  6.5987]\n",
      " [  3.8166]]\n",
      "theta [[-5. ]\n",
      " [ 1.3]]\n"
     ]
    }
   ],
   "source": [
    "print 'X', X[:10]\n",
    "print 'y', y[:10]\n",
    "print 'theta', theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "$$J(\\theta)=\\dfrac{1}{2|\\vec y|}\\left(X\\theta-\\vec{y}\\right)^T\\left(X\\theta-\\vec{y}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def JMx(theta, X, y):\n",
    "    m = len(y)\n",
    "    J = 1.0/(2.0*m)*((X*theta-y).T*(X*theta-y))\n",
    "    return J.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\Large J(\\theta) = 4.5885$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = JMx(theta, X, y) \n",
    "display(Math(r'\\Large J(\\theta) = %.4f' % error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we count derivatives?\n",
    "Let's count gradinet:\n",
    "$$\\nabla J(\\theta) = \\frac{1}{|\\vec y|} X^T\\left(X\\theta-\\vec y\\right)$$\n",
    "\n",
    "## Gradinet Descent (vectorized)\n",
    "$$ \\theta = \\theta - \\alpha \\nabla J(\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 1. \n",
    "Implement vectorized GD Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal matrix method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count $\\hat\\theta$ using this equation:\n",
    "$$\\theta = (X^TX)^{-1}X^T \\vec y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 2.\n",
    "Implement normal matrix method and check if $\\theta$ vector is the same in GD method and normal Matrix method\n",
    "Use pinv for computing inverse matrix. It's more numerical stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Gradient Method | Normal matrix|\n",
    "|----------------|--------------|\n",
    "|need to choose $\\alpha$| no need to choose $\\alpha$|\n",
    "|needs many iterations | no iterations |\n",
    "|it works for large amount of features (x)| slow for large amount of features (x)\n",
    "| | we need to count inverse matrix |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 3.\n",
    "Use library scikit-learn (normal matrix and gradient method) to fit model & predict y for x = 1, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    SGDRegressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
